{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CLM_MLM_TLM.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "qzJ6BdI5brF6",
        "LOUY1Ig2uzoB",
        "aQzBLQCVu1ky"
      ],
      "authorship_tag": "ABX9TyMLYaIiL+8dix9CHIiMi+9M",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deterministic-algorithms-lab/NLP-Journey/blob/main/LanguageModelling/CLM_MLM_TLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fmUfNCoEbSXE"
      },
      "source": [
        "!git clone https://github.com/deterministic-algorithms-lab/NLP-Journey\n",
        "%cd NLP-Journey\n",
        "!pip install -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzQKXqR7bt0m"
      },
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import haiku as hk\n",
        "import optax\n",
        "\n",
        "import numpy as np\n",
        "from functools import partial"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ai8N55L2cVlp"
      },
      "source": [
        "import src.DataLoaders.tfds as tfdl\n",
        "from src.Tokenizers.hf_tokenizer import LM_Tokenizer\n",
        "from src.model.transformer import LogitsTransformer\n",
        "from src.optimizers.adam import get_adam_opt\n",
        "from src.Tokenizers.masking_utils import mask_batch_mlm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzJ6BdI5brF6"
      },
      "source": [
        "## Setting Up Config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7Bgmf_3cr4i"
      },
      "source": [
        "config = {\n",
        "          #Data Parameters\n",
        "          'max_length' : 512, \n",
        "          'batch_size' : 4,\n",
        "\n",
        "          #Model Parameters\n",
        "          'intermediate_size' : 3072,\n",
        "          'n_heads' : 12,\n",
        "          'n_layers' : 12,\n",
        "          'hidden_size' : 768,\n",
        "          'd_model' : 768,                                                      #same as hidden_size\n",
        "          \n",
        "          #Embeddings Parameters\n",
        "          'embed_dropout_rate' : 0.1,\n",
        "          'lang2id' : {'en' : 1, 'ne' : 2},\n",
        "          \n",
        "          #MHA parameters\n",
        "          'attention_drop_rate' : 0.1,\n",
        "          \n",
        "          #MLP parameters\n",
        "          'fully_connected_drop_rate' : 0.1,\n",
        "          \n",
        "          #Training Parameters\n",
        "          'learning_rate' : 1e-5,\n",
        "          'max_grad_norm' : 1.0,\n",
        "          'l2' : 0.1,\n",
        "          'n_epochs' : 5,\n",
        "          'n_examples' : 25000,\n",
        "\n",
        "          #Task no.\n",
        "          'mlm' : 0,\n",
        "          'clm' : 1,\n",
        "          }\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOUY1Ig2uzoB"
      },
      "source": [
        "## Getting Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HmC0IPQSbmF4"
      },
      "source": [
        "imdb_ds = tfdl.load_tf_dataset(config, training=True, split='train', n_epochs=3, n_examples=25000)                                  #For MLM, CLM\n",
        "flores_neen = tfdl.load_tf_dataset(config, training=True, split='test', n_epochs=50, n_examples=-1, name='flores/neen_plain_text')   #For TLM"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQzBLQCVu1ky"
      },
      "source": [
        "## Training Tokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bl2jwUcauODk"
      },
      "source": [
        "def enne_iter():\n",
        "    for elem1, elem2 in zip(flores_neen, imdb_ds):\n",
        "        yield elem1['en']\n",
        "        yield elem1['ne']\n",
        "        yield elem1['ne']\n",
        "        yield elem2['text']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1oWL0v1ffEus"
      },
      "source": [
        "lm_tokeniser = LM_Tokenizer(config)\n",
        "lm_tokeniser.train_tokenizer(binary_iterator=enne_iter())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCuyUhY4uuOg"
      },
      "source": [
        "print(lm_tokeniser.tokenizer.get_vocab())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bw8KWNrKzIf0"
      },
      "source": [
        "### Updating Config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZodbGMNEwrrI"
      },
      "source": [
        "config['vocab_size'] = lm_tokeniser.tokenizer.get_vocab_size()\n",
        "\n",
        "#Tokenization ids  \n",
        "config['mask_id'] = lm_tokeniser.tokenizer.token_to_id(\"<mask>\")\n",
        "config['pad_id'] = lm_tokeniser.tokenizer.token_to_id(\"<pad>\")\n",
        "config['sos_id'] = lm_tokeniser.tokenizer.token_to_id(\"<s>\")\n",
        "config['eos_id'] = lm_tokeniser.tokenizer.token_to_id(\"</s>\")\n",
        "config = hk.data_structures.to_immutable_dict(config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fh9xFNpDzLsv"
      },
      "source": [
        "## Language Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9TyugkbUvGpX"
      },
      "source": [
        "def logits_fn(masked_token_ids, lang_ids=None, training=True, task=config['mlm']):\n",
        "     logits = LogitsTransformer(config)(masked_token_ids, lang_ids, \n",
        "                                       training=training, \n",
        "                                       is_autoregressive=(task==config['clm']))\n",
        "     return logits\n",
        "\n",
        "key, subkey = jax.random.split( jax.random.PRNGKey(42) )\n",
        "pure_logits_fn = hk.transform(logits_fn)\n",
        "\n",
        "token_encoding = lm_tokeniser.batch_encode_plus(['sample sentence', 'Another one!', \"we need to make\", \"this equal to batch size\"])\n",
        "\n",
        "token_ids = np.asarray(lm_tokeniser.get_token_ids(token_encoding), dtype=np.int16)\n",
        "lang_ids = np.asarray(lm_tokeniser.get_lang_ids(token_encoding), dtype=np.int16)\n",
        "\n",
        "masked_token_ids, original_batch = mask_batch_mlm(subkey, config, token_ids)\n",
        "\n",
        "key, subkey = jax.random.split(key)\n",
        "params = pure_logits_fn.init(subkey, masked_token_ids, lang_ids=lang_ids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ig_KhCk2zhzv"
      },
      "source": [
        "def loss(params, key, original_batch, masked_token_ids, lang_ids=None, task=config['mlm']) :\n",
        "    \n",
        "    logits = pure_logits_fn.apply(params, key, \n",
        "                                  masked_token_ids, lang_ids=lang_ids,\n",
        "                                  training=True, task=task)\n",
        "    \n",
        "    if task==config['clm']:\n",
        "        logits = logits[:,:-1,:]\n",
        "        original_batch = original_batch[:,1:]\n",
        "        logits_mask = jnp.ones_like(original_batch)\n",
        "    else :\n",
        "        logits_mask = (masked_token_ids==config['mask_id'])\n",
        "        \n",
        "    logits = jax.vmap(jnp.multiply, (None,2), 2)(logits_mask,logits)\n",
        "    labels = hk.one_hot(original_batch, config['vocab_size'])\n",
        "    softmax_xent = -jnp.sum(labels*jax.nn.log_softmax(logits))\n",
        "    \n",
        "    total_masks = jnp.sum(logits_mask)\n",
        "    softmax_xent /= total_masks\n",
        "    return softmax_xent\n",
        "\n",
        "@partial(jax.jit, static_argnums=(5,))\n",
        "def update(params, rng, opt_state, original_batch, masked_token_ids, task, lang_ids=None):\n",
        "    batch_loss, grad = jax.value_and_grad(loss)(params, rng, original_batch, masked_token_ids, \n",
        "                                                lang_ids=lang_ids, task=task)\n",
        "    updates, opt_state = opt.update(grad, opt_state)\n",
        "    new_params = optax.apply_updates(params, updates)\n",
        "    return new_params, opt_state, batch_loss\n",
        "\n",
        "@partial(jax.jit, static_argnums=(5,))\n",
        "def accuracy(params, rng, original_batch, masked_token_ids, task, lang_ids=None):\n",
        "    logits = LogitsTransformer(config)(masked_token_ids, lang_ids, \n",
        "                                       training=True, \n",
        "                                       is_autoregressive=(task==config['clm']))\n",
        "    if task=='clm':\n",
        "        logits = logits[:,:-1,:]\n",
        "        original_batch = original_batch[:,1:]\n",
        "        logits_mask = jnp.ones_like(original_batch)\n",
        "    \n",
        "    else :\n",
        "        logits_mask = (masked_token_ids==config['mask_id'])\n",
        "    total_masks = jnp.sum(logits_mask)\n",
        "\n",
        "    if total_masks: softmax_xent /= total_masks\n",
        "    return jnp.sum((jnp.argmax(logits, axis=-1)==original_batch)*logits_mask)/total_masks\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4C2NgvBPteSs"
      },
      "source": [
        "## Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOUtfkolthMo"
      },
      "source": [
        "opt = get_adam_opt(config)\n",
        "opt_state = opt.init(params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXSydQ3XpPOF"
      },
      "source": [
        "## Training Loops"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABWRR1aF3kE1"
      },
      "source": [
        "### MLM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-aBoNsatb5q"
      },
      "source": [
        "losses = []\n",
        "for step, train_batch in enumerate(imdb_ds):\n",
        "    if step%100==0:\n",
        "        print(f'[Step {step}]')\n",
        "    \n",
        "    token_encoding = lm_tokeniser.batch_encode_plus(train_batch['text'])\n",
        "    token_ids = np.asarray(lm_tokeniser.get_token_ids(token_encoding), dtype=np.int16)\n",
        "    lang_ids = np.asarray(lm_tokeniser.get_lang_ids(token_encoding), dtype=np.int16)\n",
        "\n",
        "    key, subkey = jax.random.split(key)\n",
        "    masked_token_ids, original_batch = mask_batch_mlm(subkey, config, token_ids)\n",
        "    \n",
        "    key, subkey = jax.random.split(key)\n",
        "    params, opt_state, batch_loss = update(params, subkey, opt_state,\n",
        "                                           original_batch, masked_token_ids, \n",
        "                                           config['mlm'], lang_ids=lang_ids)\n",
        "    losses.append(batch_loss)\n",
        "\n",
        "    if step%100==0 and step!=0:\n",
        "        print(sum(losses)/100)\n",
        "        losses = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCIzL0sm_IKU"
      },
      "source": [
        "### TLM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNStIFyE_KFt"
      },
      "source": [
        "losses = []\n",
        "for step, train_batch in enumerate(flores_neen):\n",
        "    if step%100==0:\n",
        "        print(f'[Step {step}]')\n",
        "    \n",
        "    token_encoding = lm_tokeniser.batch_encode_plus(train_batch['en'], train_batch['ne'])\n",
        "    token_ids = np.asarray(lm_tokeniser.get_token_ids(token_encoding), dtype=np.int16)\n",
        "    lang_ids = np.asarray(lm_tokeniser.get_lang_ids(token_encoding), dtype=np.int16)\n",
        "\n",
        "    key, subkey = jax.random.split(key)\n",
        "    masked_token_ids, original_batch = mask_batch_mlm(subkey, config, token_ids)\n",
        "\n",
        "    key, subkey = jax.random.split(key)\n",
        "    params, opt_state, batch_loss = update(params, subkey, opt_state, \n",
        "                                           original_batch, masked_token_ids,\n",
        "                                           config['mlm'], lang_ids=lang_ids,)\n",
        "    losses.append(batch_loss)\n",
        "    \n",
        "    if step%100==0 and step!=0:\n",
        "        print(sum(losses)/100)\n",
        "        losses = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXyG4YNAHj6r"
      },
      "source": [
        "### CLM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "au37Dg1SHlPk"
      },
      "source": [
        "losses = []\n",
        "for step, train_batch in enumerate(imdb_ds):\n",
        "    if step%100==0:\n",
        "        print(f'[Step {step}]')\n",
        "    \n",
        "    token_encoding = lm_tokeniser.batch_encode_plus(train_batch['text'])\n",
        "    token_ids = np.asarray(lm_tokeniser.get_token_ids(token_encoding), dtype=np.int16)\n",
        "    lang_ids = np.asarray(lm_tokeniser.get_lang_ids(token_encoding), dtype=np.int16)\n",
        "\n",
        "    \n",
        "    key, subkey = jax.random.split(key)\n",
        "    masked_token_ids, original_batch = mask_batch_mlm(subkey, config, token_ids)\n",
        "\n",
        "    key, subkey = jax.random.split(key)\n",
        "    params, opt_state, batch_loss = update(params, subkey, opt_state,\n",
        "                                           original_batch, masked_token_ids, \n",
        "                                           config['clm'], lang_ids=lang_ids)\n",
        "    losses.append(batch_loss)\n",
        "\n",
        "    if step%100==0 and step!=0:\n",
        "        print(sum(losses)/100)\n",
        "        losses = []"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}